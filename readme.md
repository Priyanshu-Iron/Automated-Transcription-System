# Whisper Transcription System ğŸ™ï¸ğŸ“

An automated transcription system built with OpenAI's Whisper model, featuring recursive file scanning, real-time directory monitoring, and a simple GUI. This project transcribes audio and video files in a specified directory and saves the results as text files, with an optional monitoring mode for new file additions. ğŸš€

## Features âœ¨
- **Recursive File Scanning** ğŸŒ: Processes audio (`.mp3`, `.wav`, `.aac`, `.m4a`) and video (`.mp4`, `.mkv`, `.mov`, `.flv`) files in a directory and its subdirectories.
- **Automatic Transcription** ğŸ¤: Uses OpenAI's Whisper "base" model for high-quality speech-to-text conversion.
- **Real-Time Monitoring** ğŸ‘€: Detects and transcribes new files added to the monitored directory using `watchdog`.
- **GUI Interface** ğŸ–¥ï¸: Built with Tkinter, includes directory selection, file upload, and a "Processing..." animation.
- **Optimization** âš¡: Skips previously processed files using a persistent `processed_files.txt` log.

## Demo ğŸ¥
- Transcribes a 4.4 MB video file (`videoplayback.mp4`) in approximately 32 seconds on a CPU. â±ï¸
- Example output:
  ```
  22:24:16 - Processing: videoplayback.mp4
  22:24:49 - Completed: videoplayback.mp4 (took 32.53 seconds)
  ```

## Prerequisites âœ…
- **Python** ğŸ: Version 3.8+ (tested with 3.12 via Anaconda).
- **System** ğŸ’»: macOS (tested), Linux/Windows compatible with adjustments.
- **FFmpeg** ğŸ¬: Required for video file support (e.g., `.mp4`).

## Installation ğŸ› ï¸

### 1. Clone the Repository ğŸ“¦
```bash
git clone https://github.com/Priyanshu-Iron/Automated-Transcription-System
cd whisper-transcription-system
```

### 2. Install FFmpeg ğŸ§
- **macOS** (using Homebrew):
  ```bash
  brew install ffmpeg
  ```
- **Linux**:
  ```bash
  sudo apt-get install ffmpeg
  ```
- **Windows**: Download from [ffmpeg.org](https://ffmpeg.org/download.html) and add to PATH.

### 3. Install Python Dependencies ğŸ“š
Using `pip` (preferably in a virtual environment):
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Requirements ğŸ“‹
```plaintext
# requirements.txt
openai-whisper>=20231117        # OpenAI's Whisper for audio/video transcription
watchdog>=2.3.1                 # For real-time directory monitoring
tk>=0.1.0                       # Tkinter for GUI (usually bundled with Python)
torch>=2.0.0                    # PyTorch, Whisper's core ML framework
numpy>=1.20.0                   # Numerical operations, used by Whisper/Torch
tqdm>=4.64.1                    # Progress bars in Whisper
# Note: Install FFmpeg separately (see above)
```

## Usage ğŸš€

1. **Run the Application** â–¶ï¸:
   ```bash
   python main.py
   ```

2. **GUI Options** ğŸ–±ï¸:
   - **Select Directory** ğŸ“: Choose a folder to scan and transcribe existing files.
   - **Upload File** â¬†ï¸: Select a single file to transcribe (requires a directory selected first).
   - **Start Monitoring** ğŸ”: Watch the directory for new files and transcribe them automatically.
   - **Stop Monitoring** â¹ï¸: Pause real-time monitoring.

3. **Output** ğŸ“„:
   - Transcriptions are saved as `.txt` files next to the original files (e.g., `videoplayback.txt`).
   - GUI shows status updates and a processing animation.

4. **Example** ğŸŒŸ:
   - Place `videoplayback.mp4` in `/path/to/Samples/`.
   - Select `/path/to/Samples/` as the directory.
   - Output:
     ```
     22:24:16 - Scanning existing files...
     22:24:16 - Processing: videoplayback.mp4
     22:24:49 - Completed: videoplayback.mp4 (took 32.53 seconds)
     ```

## Project Structure ğŸ—‚ï¸
```
whisper-transcription-system/
â”œâ”€â”€ main.py              # Entry point, ties GUI and monitoring together
â”œâ”€â”€ gui.py              # Tkinter GUI with animation
â”œâ”€â”€ transcriber.py      # Whisper transcription logic
â”œâ”€â”€ file_monitor.py     # File scanning and real-time monitoring
â”œâ”€â”€ requirements.txt    # Python dependencies
â”œâ”€â”€ processed_files.txt # Tracks processed files (generated on run)
â”œâ”€â”€ Samples/            # Example directory for test files (e.g., videoplayback.mp4)
â””â”€â”€ README.md           # This file
```

## Performance ğŸ“ˆ
- **Transcription Time** â³: ~32 seconds for a 4.4 MB video (~30-60s duration) on a CPU with FP32.
- **Optimizations** âš™ï¸: Skips processed files, persists state across runs.

## Troubleshooting ğŸ
- **No `ffmpeg` Error** ğŸš«: Ensure FFmpeg is installed and in your PATH (`ffmpeg -version`).
- **Missing Transcript** â“: Check file permissions in the directory and verify the file path.
- **Slow Performance** ğŸ¢: Use a smaller Whisper model (e.g., "tiny") or enable GPU/MPS support.

## Future Improvements ğŸ”®
- Add transcript preview in the GUI. ğŸ‘ï¸
- Support for multiple languages via Whisper options. ğŸŒ
- Parallel processing for batch transcription. âš¡
- Progress bar instead of dot animation. ğŸ“Š

## License ğŸ“œ
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details (add one if desired).

## Acknowledgments ğŸ™
- Built with guidance from xAIâ€™s Grok assistant. ğŸ¤–
- Uses OpenAIâ€™s Whisper for transcription. ğŸ™ï¸

---
**Author**: Priyanshu  
**GitHub**: [Priyanshu-Iron](https://github.com/Priyanshu-Iron)
```

---
