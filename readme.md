# Whisper Transcription System 🎙️📝

An automated transcription system built with OpenAI's Whisper model, featuring recursive file scanning, real-time directory monitoring, and a simple GUI. This project transcribes audio and video files in a specified directory and saves the results as text files, with an optional monitoring mode for new file additions. 🚀

## Features ✨
- **Recursive File Scanning** 🌐: Processes audio (`.mp3`, `.wav`, `.aac`, `.m4a`) and video (`.mp4`, `.mkv`, `.mov`, `.flv`) files in a directory and its subdirectories.
- **Automatic Transcription** 🎤: Uses OpenAI's Whisper "base" model for high-quality speech-to-text conversion.
- **Real-Time Monitoring** 👀: Detects and transcribes new files added to the monitored directory using `watchdog`.
- **GUI Interface** 🖥️: Built with Tkinter, includes directory selection, file upload, and a "Processing..." animation.
- **Optimization** ⚡: Skips previously processed files using a persistent `processed_files.txt` log.

## Demo 🎥
- Transcribes a 4.4 MB video file (`videoplayback.mp4`) in approximately 32 seconds on a CPU. ⏱️
- Example output:
  ```
  22:24:16 - Processing: videoplayback.mp4
  22:24:49 - Completed: videoplayback.mp4 (took 32.53 seconds)
  ```

## Prerequisites ✅
- **Python** 🐍: Version 3.8+ (tested with 3.12 via Anaconda).
- **System** 💻: macOS (tested), Linux/Windows compatible with adjustments.
- **FFmpeg** 🎬: Required for video file support (e.g., `.mp4`).

## Installation 🛠️

### 1. Clone the Repository 📦
```bash
git clone https://github.com/Priyanshu-Iron/Automated-Transcription-System
cd whisper-transcription-system
```

### 2. Install FFmpeg 🎧
- **macOS** (using Homebrew):
  ```bash
  brew install ffmpeg
  ```
- **Linux**:
  ```bash
  sudo apt-get install ffmpeg
  ```
- **Windows**: Download from [ffmpeg.org](https://ffmpeg.org/download.html) and add to PATH.

### 3. Install Python Dependencies 📚
Using `pip` (preferably in a virtual environment):
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### Requirements 📋
```plaintext
# requirements.txt
openai-whisper>=20231117        # OpenAI's Whisper for audio/video transcription
watchdog>=2.3.1                 # For real-time directory monitoring
tk>=0.1.0                       # Tkinter for GUI (usually bundled with Python)
torch>=2.0.0                    # PyTorch, Whisper's core ML framework
numpy>=1.20.0                   # Numerical operations, used by Whisper/Torch
tqdm>=4.64.1                    # Progress bars in Whisper
# Note: Install FFmpeg separately (see above)
```

## Usage 🚀

1. **Run the Application** ▶️:
   ```bash
   python main.py
   ```

2. **GUI Options** 🖱️:
   - **Select Directory** 📁: Choose a folder to scan and transcribe existing files.
   - **Upload File** ⬆️: Select a single file to transcribe (requires a directory selected first).
   - **Start Monitoring** 🔍: Watch the directory for new files and transcribe them automatically.
   - **Stop Monitoring** ⏹️: Pause real-time monitoring.

3. **Output** 📄:
   - Transcriptions are saved as `.txt` files next to the original files (e.g., `videoplayback.txt`).
   - GUI shows status updates and a processing animation.

4. **Example** 🌟:
   - Place `videoplayback.mp4` in `/path/to/Samples/`.
   - Select `/path/to/Samples/` as the directory.
   - Output:
     ```
     22:24:16 - Scanning existing files...
     22:24:16 - Processing: videoplayback.mp4
     22:24:49 - Completed: videoplayback.mp4 (took 32.53 seconds)
     ```

## Project Structure 🗂️
```
whisper-transcription-system/
├── main.py              # Entry point, ties GUI and monitoring together
├── gui.py              # Tkinter GUI with animation
├── transcriber.py      # Whisper transcription logic
├── file_monitor.py     # File scanning and real-time monitoring
├── requirements.txt    # Python dependencies
├── processed_files.txt # Tracks processed files (generated on run)
├── Samples/            # Example directory for test files (e.g., videoplayback.mp4)
└── README.md           # This file
```

## Performance 📈
- **Transcription Time** ⏳: ~32 seconds for a 4.4 MB video (~30-60s duration) on a CPU with FP32.
- **Optimizations** ⚙️: Skips processed files, persists state across runs.

## Troubleshooting 🐞
- **No `ffmpeg` Error** 🚫: Ensure FFmpeg is installed and in your PATH (`ffmpeg -version`).
- **Missing Transcript** ❓: Check file permissions in the directory and verify the file path.
- **Slow Performance** 🐢: Use a smaller Whisper model (e.g., "tiny") or enable GPU/MPS support.

## Future Improvements 🔮
- Add transcript preview in the GUI. 👁️
- Support for multiple languages via Whisper options. 🌍
- Parallel processing for batch transcription. ⚡
- Progress bar instead of dot animation. 📊

## License 📜
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details (add one if desired).

## Acknowledgments 🙏
- Built with guidance from xAI’s Grok assistant. 🤖
- Uses OpenAI’s Whisper for transcription. 🎙️

---
**Author**: Priyanshu  
**GitHub**: [Priyanshu-Iron](https://github.com/Priyanshu-Iron)
```

---
